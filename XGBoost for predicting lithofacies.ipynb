{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import statistics as stat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import shap\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# make a binary prediction (above or below median) =1\n",
    "# or make a continuum of predictions ==0\n",
    "get_bin = 1\n",
    "\n",
    "# decide to get shap values or not\n",
    "get_shap = 1\n",
    "\n",
    "# filename of the data\n",
    "fname = 'calculate TOC-1.txt'\n",
    "# features to use for predictions\n",
    "# well_n depth lith DT24/DTC GR M2R2 M2R3 M2R6 M2R9 M2RX ZDEN CNCF PE SP clay silica carbonate\n",
    "features = ['DT','CNCF', 'ZDEN','GR', 'LLD','LLS','DT*CNCF','DT*ZDEN','DT*GR','DT*LLD','DT*LLS',\n",
    "            'CNCF*ZDEN','CNCF*GR', 'CNCF*LLD','CNCF*LLS','ZDEN*GR','ZDEN*LLD','ZDEN*LLS','GR*LLD','GR*LLS','LLD*LLS']\n",
    "#features = ['DT','CNCF', 'ZDEN','GR','LLS','DT*CNCF','DT*ZDEN','CNCF*ZDEN','CNCF*LLD','CNCF*LLS']\n",
    "#features = ['DT','CNCF', 'ZDEN','GR','LLS']\n",
    "\n",
    "\n",
    "numf = len(features)\n",
    "\n",
    "# list of lab measurements to predict\n",
    "preds = ['TOC']\n",
    "\n",
    "\n",
    "for pred in preds:\n",
    "    # filename of where to write the output\n",
    "    score_txt = 'borehole_scores_'+pred+'_bin_'+str(get_bin)+'.txt'\n",
    "    shap_txt = 'borehole_shap_'+pred+'_bin_'+str(get_bin)+'.txt'\n",
    "    \n",
    "    # read in the data\n",
    "    df = pd.read_csv(fname, delim_whitespace=True)\n",
    "    # drop the data that has nan in some row (where there are no measurements)\n",
    "    df = df.dropna().copy()\n",
    "    \n",
    "    # scale the features\n",
    "    df_scale = df.copy()\n",
    "    trans = RobustScaler()\n",
    "    df_scale[features] = trans.fit_transform(df_scale[features].values)\n",
    "    \n",
    "    # separate the predictions into above and below the mean or median of the population\n",
    "    predvs = df_scale[pred].values\n",
    "    #mid = stat.mean(predvs)\n",
    "    \n",
    "    print(\"predicting: \", pred)\n",
    "    if get_bin:\n",
    "        mid = stat.median(predvs)\n",
    "        pred_bin = [1 if e>mid else 0 for i, e in enumerate(predvs)]\n",
    "        df_scale[\"predict\"] = pred_bin\n",
    "        \n",
    "        # count the number in each category\n",
    "        num1 = sum(pred_bin)\n",
    "        num0 = len(pred_bin)-num1\n",
    "        print(\"number of samples in prediction classes:\", num1, num0)\n",
    "    else:\n",
    "        df_scale[\"predict\"] = predvs\n",
    "        vmin = min(predvs)\n",
    "        vmax= max(predvs)\n",
    "        print(\"range of predictions:\", vmin, vmax)\n",
    "    \n",
    "    \n",
    "    score_str = \"prediction iteration rmse_train r2_train rmse_test r2_test\\n\"\n",
    "    shp_str = \"feat_str it feat_num mean_shap \\n\";\n",
    "        \n",
    "    # make 10 different models that differ in how the training/testing data\n",
    "    # is split\n",
    "    it=0\n",
    "    while it<20:\n",
    "        # split into training and testing data set\n",
    "        inds = np.random.uniform(0, 1, len(df_scale)) <= .80 \n",
    "        \n",
    "        df_scale['is_train'] = inds\n",
    "        train, test = df_scale[df_scale['is_train']==True], df_scale[df_scale['is_train']==False]\n",
    "        \n",
    "        x_train = train[features]\n",
    "        y_train = train[pred]\n",
    "          \n",
    "        x_test = test[features]\n",
    "        y_test = test[pred]\n",
    "        \n",
    "        \n",
    "        # weighted features\n",
    "        # add features to explain subsets of data\n",
    "        X = df_scale[features]\n",
    "        y = df_scale[pred]\n",
    "        data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "        \n",
    "        # max_depth: Maximum depth of a tree. \n",
    "        # Increasing this value will make the model more complex and more likely to overfit\n",
    "        # colsample_bytree: subsample ratio of columns when constructing each tree\n",
    "        # alpha: L1 regularization term on weights. Default=0\n",
    "        # Increasing this value will make model more conservative\n",
    "        # learning_rate, eta = Typical final values to be used: 0.01-0.2\n",
    "        xgb_clf = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "        #xgb_clf = xgb.XGBClassifier(objective ='multi:softmax')\n",
    "        #parameters = {'colsample_bytree':[0.7, 0.8, 0.9], 'alpha':[0, 3, 5], \n",
    "        #      'learning_rate': [0.1, 0.2, 0.3],\n",
    "        #      'n_estimators': [100, 200, 300], 'max_depth':[3, 4, 5, 6]}\n",
    "        parameters = {'colsample_bytree':[0.8, 0.9], 'alpha':[3, 5], \n",
    "              'learning_rate': [0.1],\n",
    "              'n_estimators': [100, 200, 300], 'max_depth':[4, 5, 6]}\n",
    "\n",
    "        \n",
    "        grid_search = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=10, n_jobs=-1)\n",
    "        \n",
    "        grid_search.fit(x_train, y_train)\n",
    "        xg_reg = grid_search.best_estimator_\n",
    "        \n",
    "        preds = xg_reg.predict(x_test)\n",
    "        preds_train = xg_reg.predict(x_train)\n",
    "        \n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        \n",
    "        r2_test = r2_score(y_test, preds)\n",
    "        r2_train = r2_score(y_train, preds_train)\n",
    "        \n",
    "        score_str = score_str+\"%s %d %.2f %.2f %.2f %.2f \\n\" % (pred, it, rmse_train, r2_train, rmse_test, r2_test)\n",
    "    \n",
    "        print(score_str)\n",
    "    \n",
    "        if r2_test>0.68:\n",
    "            # test this model on the data without lab measurement\n",
    "            # 1000 rows of data: 1000 prediction of carbonate for 1 model\n",
    "            #xgb_clf.save_model('001.model')\n",
    "            pickle.dump(xg_reg, open(\"pima_TOC_1.pickle.dat\", \"wb\"))\n",
    "            #dfu = pd.read_csv('data_all_predict.txt', delim_whitespace=True)\n",
    "            #dfu_scale = dfu.copy()\n",
    "            #trans = RobustScaler()\n",
    "            #dfu_scale[features] = trans.fit_transform(dfu_scale[features].values)\n",
    "            #x_unseen = dfu_scale[features]\n",
    "            #clay = xg_reg.predict(x_unseen)\n",
    "            #file = open('model_result.txt', mode='w')\n",
    "            #file.write(str(clay)) \n",
    "            #file.close\n",
    "            #print(clay)\n",
    "\n",
    "            x_train_best = train[features]\n",
    "            y_train_best = train[pred]            \n",
    "            x_test_best = test[features]\n",
    "            y_test_best = test[pred]   \n",
    "            best_train = np.append(x_train_best,y_train_best[:,None],axis=1)\n",
    "            best_test = np.append(x_test_best,y_test_best[:,None],axis=1)\n",
    "            \n",
    "            \n",
    "            np.savetxt('best train data_clay.txt',(best_train))\n",
    "            np.savetxt('best test data_clay.txt',(best_test))\n",
    "                      \n",
    "\n",
    "            #file = open('best train data.txt', mode='w')\n",
    "            #file.write (str(x_train_best))\n",
    "            #file.write (str(y_train_best))\n",
    "            #file.close\n",
    "            \n",
    "\n",
    "    \n",
    "        if get_shap:\n",
    "            shap_vals = shap.TreeExplainer(xg_reg).shap_values(train[features], check_additivity=False)\n",
    "            \n",
    "            shap.summary_plot(shap_vals, train[features], plot_type=\"bar\")\n",
    "            shap_comb = shap_vals.transpose()\n",
    "            \n",
    "            shap_mean = []\n",
    "            num_f = len(shap_comb)\n",
    "            for fi in range(len(shap_comb)):              \n",
    "                vabs = abs(shap_comb[fi])\n",
    "                v_mean = stat.mean(vabs)       \n",
    "                shap_mean.append(v_mean)\n",
    "        \n",
    "            shapl = list(zip(features, shap_mean, range(1, numf+1)))\n",
    "            shapl.sort(key=lambda tup: tup[1], reverse=True)\n",
    "                \n",
    "            shps = \"\"\n",
    "            for ft in shapl:\n",
    "                shps = shps+ft[0]+\" \"+str(it)+\" \"+str(ft[2])+\" \"+str(round(ft[1], 4))+\"\\n\"\n",
    "            \n",
    "            print(shps)\n",
    "            shp_str = shp_str+shps\n",
    "                    \n",
    "        it=it+1\n",
    "\n",
    "    # write scores to textfile\n",
    "    f= open(score_txt, \"w\")\n",
    "    f.write(score_str)\n",
    "    f.close()\n",
    "    print(score_txt)\n",
    "    \n",
    "    if get_shap:\n",
    "        # write scores to textfile\n",
    "        f= open(shap_txt, \"w\")\n",
    "        f.write(shp_str)\n",
    "        f.close()\n",
    "        print(shap_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import shap\n",
    "\n",
    "features = ['DT','CNCF', 'ZDEN','GR', 'LLD','LLS','DT*CNCF','DT*ZDEN','DT*GR','DT*LLD','DT*LLS',\n",
    "            'CNCF*ZDEN','CNCF*GR', 'CNCF*LLD','CNCF*LLS','ZDEN*GR','ZDEN*LLD','ZDEN*LLS','GR*LLD','GR*LLS','LLD*LLS']\n",
    "#features = ['DT24/DTC', 'DT/GR','GR', 'GR_silica','GR_clay','M2R2','M2R3','M2R6','M2R9','M2RX',\n",
    "#            'ZDEN', 'CNCF','CNCF_clay','TM','TP','TN','PE','SP']\n",
    "\n",
    "# load model from file\n",
    "load_model = pickle.load(open(\"pima_TOC_1.pickle.dat\", \"rb\"))\n",
    "\n",
    "dfu = pd.read_csv('calculate TOC-3.txt', delim_whitespace=True)\n",
    "dfu_scale = dfu.copy()\n",
    "trans = RobustScaler()\n",
    "dfu_scale[features] = trans.fit_transform (dfu_scale[features].values)\n",
    "x_unseen = dfu_scale[features]\n",
    "                     \n",
    "# make predictions for test data\n",
    "y_pred = load_model.predict(x_unseen)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "print (predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
